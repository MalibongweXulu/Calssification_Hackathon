{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for this challenge is the NCHLT Text Corpora collected by the South African Department of Arts and Culture & Centre for Text Technology (CTexT, North-West University, South Africa). The training set was improved through additional cleaning done by Praekelt.\n",
    "\n",
    "The data is in the form Language ID, Text. The text is in various states of cleanliness. Some NLP techniques will be necessary to clean up the data.\n",
    "\n",
    "\n",
    "**File descriptions**\n",
    "\n",
    "train_set.csv - the training set\n",
    "\n",
    "test_set.csv - the test set\n",
    "\n",
    "sample_submission.csv - a sample submission file in the correct format\n",
    "\n",
    "\n",
    "**Language IDs**\n",
    "\n",
    "afr - Afrikaans\n",
    "\n",
    "eng - English\n",
    "\n",
    "nbl - isiNdebele\n",
    "\n",
    "nso - Sepedi\n",
    "\n",
    "sot - Sesotho\n",
    "\n",
    "ssw - siSwati\n",
    "\n",
    "tsn - Setswana\n",
    "\n",
    "tso - Xitsonga\n",
    "\n",
    "ven - Tshivenda\n",
    "\n",
    "xho - isiXhosa\n",
    "\n",
    "zul - isiZulu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_set.csv')\n",
    "test = pd.read_csv('test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang_id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xho</td>\n",
       "      <td>umgaqo-siseko wenza amalungiselelo kumaziko ax...</td>\n",
       "      <td>umgaqo siseko wenza amalungiselelo kumaziko ax...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xho</td>\n",
       "      <td>i-dha iya kuba nobulumko bokubeka umsebenzi na...</td>\n",
       "      <td>dha iya kuba nobulumko bokubeka umsebenzi naph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng</td>\n",
       "      <td>the province of kwazulu-natal department of tr...</td>\n",
       "      <td>province kwazulu natal department transport in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nso</td>\n",
       "      <td>o netefatša gore o ba file dilo ka moka tše le...</td>\n",
       "      <td>o netefatša gore o ba file dilo ka moka tše le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ven</td>\n",
       "      <td>khomishini ya ndinganyiso ya mbeu yo ewa maana...</td>\n",
       "      <td>khomishini ya ndinganyiso ya mbeu yo ewa maana...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang_id                                               text  \\\n",
       "0     xho  umgaqo-siseko wenza amalungiselelo kumaziko ax...   \n",
       "1     xho  i-dha iya kuba nobulumko bokubeka umsebenzi na...   \n",
       "2     eng  the province of kwazulu-natal department of tr...   \n",
       "3     nso  o netefatša gore o ba file dilo ka moka tše le...   \n",
       "4     ven  khomishini ya ndinganyiso ya mbeu yo ewa maana...   \n",
       "\n",
       "                                            text_new  \n",
       "0  umgaqo siseko wenza amalungiselelo kumaziko ax...  \n",
       "1  dha iya kuba nobulumko bokubeka umsebenzi naph...  \n",
       "2  province kwazulu natal department transport in...  \n",
       "3  o netefatša gore o ba file dilo ka moka tše le...  \n",
       "4  khomishini ya ndinganyiso ya mbeu yo ewa maana...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data  preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "#loading the en_core_web_sm_model\n",
    "stopwords = STOP_WORDS\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def preprocess(train):\n",
    "    #creating a Doc object\n",
    "    doc = nlp(train, disable = ['ner', 'parser'])\n",
    "    #Generating lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    #remove stopwords and non-alphabetic characters\n",
    "    a_lemma = [lemma for lemma in lemmas\n",
    "              if lemma.isalpha() and lemma not in stopwords ]\n",
    "    return ' ' .join(a_lemma)\n",
    "\n",
    "#apply preprocessing to posts\n",
    "train['text_new']= train['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eng    3000\n",
       "ssw    3000\n",
       "tsn    3000\n",
       "sot    3000\n",
       "ven    3000\n",
       "zul    3000\n",
       "tso    3000\n",
       "afr    3000\n",
       "nso    3000\n",
       "xho    3000\n",
       "nbl    3000\n",
       "Name: lang_id, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.lang_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forrest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate features and tagret variables\n",
    "y = train['lang_id']\n",
    "X = train['text_new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=2, stop_words=\"english\")\n",
    "X_vectorized = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train data to create validation dataset\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_vectorized,y,test_size=.3,shuffle=True, stratify=y, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfc = RandomForestClassifier(max_features=4, random_state=42)\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc_pred = rfc.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9964655269584973"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val, rfc_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "testx = test['text']\n",
    "test_vect = vectorizer.transform(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfc.predict(test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['lang_id'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>lang_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mmasepala, fa maemo a a kgethegileng a letlele...</td>\n",
       "      <td>ssw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Uzakwaziswa ngokufaneleko nakungafuneka eminye...</td>\n",
       "      <td>nbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Tshivhumbeo tshi fana na ngano dza vhathu.</td>\n",
       "      <td>ven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Kube inja nelikati betingevakala kutsi titsini...</td>\n",
       "      <td>ssw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Winste op buitelandse valuta.</td>\n",
       "      <td>ssw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text lang_id\n",
       "0      1  Mmasepala, fa maemo a a kgethegileng a letlele...     ssw\n",
       "1      2  Uzakwaziswa ngokufaneleko nakungafuneka eminye...     nbl\n",
       "2      3         Tshivhumbeo tshi fana na ngano dza vhathu.     ven\n",
       "3      4  Kube inja nelikati betingevakala kutsi titsini...     ssw\n",
       "4      5                      Winste op buitelandse valuta.     ssw"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['index','lang_id']].to_csv('test_rfc_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=1.0, max_depth=1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Gradient Boosting Classifier\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
    "\n",
    "gb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[881   0  19   0   0   0   0   0   0   0   0]\n",
      " [  3 829  68   0   0   0   0   0   0   0   0]\n",
      " [  1   1 799   8   0  35   2   3   0  36  15]\n",
      " [  0   0  14 860   1   0  25   0   0   0   0]\n",
      " [  0   0  14  35 833   0  17   1   0   0   0]\n",
      " [  0   7 392   2   0 478   0   4   3   0  14]\n",
      " [  0   3  16  49   1   0 831   0   0   0   0]\n",
      " [  0   0   5   5   1   0   2 873  14   0   0]\n",
      " [  0   0  12   0  12   0   4   1 871   0   0]\n",
      " [  0   5 165   4   0   0   0   1   0 715  10]\n",
      " [  0   6 416   6   0   8   0   3   0  22 439]]\n"
     ]
    }
   ],
   "source": [
    "# Create a prediction set:\n",
    "pred_gb = gb_model.predict(X_val)\n",
    "\n",
    "# Print a confusion matrix for NB Model\n",
    "print(metrics.confusion_matrix(y_val,pred_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8567063173542224"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val, pred_gb, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the test data to test the Gradient Boosting model\n",
    "test_gb = test['text']\n",
    "test_vect_gb = vectorizer.transform(test_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the sentiment using the test data\n",
    "y_pred_gb = gb_model.predict(test_vect_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a new column on the test data by using the predicted sentiment from the tweets from test data\n",
    "test['lang_id'] = y_pred_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the csv file and submit it. \n",
    "test[['index','lang_id']].to_csv('test_gb_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "lsvc = LinearSVC()\n",
    "#print(lsvc)\n",
    "\n",
    "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
    "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
    "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
    "          verbose=0)\n",
    "\n",
    "# fit the model on train data and check the model accuracy score.\n",
    "lsvc.fit(X_train, y_train)\n",
    "score = lsvc.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply a cross-validation training method to the model and check the training score.\n",
    "cv_scores = cross_val_score(lsvc, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "#Now, we can predict the test data by using the trained model. After the prediction, we'll check the accuracy level by using the confusion matrix functionfrom sklearn.metrics import confusion_matrix\n",
    "\n",
    "ypred = lsvc.predict(X_val)\n",
    "\n",
    "cm = confusion_matrix(y_val, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         afr       1.00      1.00      1.00       900\n",
      "         eng       1.00      1.00      1.00       900\n",
      "         nbl       0.99      0.99      0.99       900\n",
      "         nso       1.00      1.00      1.00       900\n",
      "         sot       1.00      1.00      1.00       900\n",
      "         ssw       1.00      1.00      1.00       900\n",
      "         tsn       1.00      1.00      1.00       900\n",
      "         tso       1.00      1.00      1.00       900\n",
      "         ven       1.00      1.00      1.00       900\n",
      "         xho       1.00      0.99      1.00       900\n",
      "         zul       0.99      0.99      0.99       900\n",
      "\n",
      "    accuracy                           1.00      9900\n",
      "   macro avg       1.00      1.00      1.00      9900\n",
      "weighted avg       1.00      1.00      1.00      9900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We can also create a classification report by using classification_report() function on predicted data to check the other accuracy metrics\n",
    "#Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_val, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using the test data to test the lsv model\n",
    "test_lsv = test['text']\n",
    "test_vecto = vectorizer.transform(test_lsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the sentiment using the test data\n",
    "y_pred_lsv = lsvc.predict(test_vecto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['lang_id'] = y_pred_lsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the csv file and submit it. \n",
    "test[['index','lang_id']].to_csv('test_lsv2_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9986868686868687\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(X_val)\n",
    "\n",
    "print(np.mean(predicted == y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[900   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 900   0   0   0   0   0   0   0   0   0]\n",
      " [  1   1 895   0   0   0   0   0   0   0   3]\n",
      " [  0   0   0 900   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 900   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 900   0   0   0   0   0]\n",
      " [  1   0   0   1   0   0 898   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 900   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 900   0   0]\n",
      " [  0   2   0   0   0   0   0   0   0 898   0]\n",
      " [  0   3   1   0   0   0   0   0   0   0 896]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_val, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the csv file and submit it. \n",
    "test[['index','lang_id']].to_csv('test_mnb_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
